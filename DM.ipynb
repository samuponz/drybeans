{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIBRARIES\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.combine import SMOTEENN\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import GridSearchCV, KFold, StratifiedKFold, RepeatedStratifiedKFold\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import sklearn.metrics as met\n",
    "#from sklearn.pipeline import Pipeline\n",
    "from imblearn.pipeline import Pipeline\n",
    "#from sklearn.pipeline import make_pipeline\n",
    "#from imblearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET\n",
    "\n",
    "df = pd.read_excel(\"DryBeanDataset/Dry_Bean_Dataset.xlsx\")\n",
    "df = df.drop_duplicates(ignore_index=True)\n",
    "df.rename(columns = {'AspectRation':'AspectRatio'}, inplace = True)\n",
    "df = df.astype({'Area': 'float64'})\n",
    "df = df.astype({'Class': 'string'})\n",
    "df.info()\n",
    "df.describe() #df[df['Class']=='BOMBAY'].describe()\n",
    "labels = ['BARBUNYA', 'BOMBAY', 'CALI', 'DERMASON', 'HOROZ', 'SEKER', 'SIRA']\n",
    "colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple', 'tab:brown', 'tab:pink']\n",
    "features = df.columns.to_list();\n",
    "features.remove('Class');\n",
    "#features = ['Area', 'Perimeter', 'MajorAxisLength', 'MinorAxisLength', 'AspectRatio', 'Eccentricity', 'ConvexArea', 'EquivDiameter', \n",
    "#            'Extent', 'Solidity', 'roundness', 'Compactness', 'ShapeFactor1', 'ShapeFactor2','ShapeFactor3', 'ShapeFactor4'];\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BOXPLOTS\n",
    "\n",
    "#df.boxplot('Area', by='Class');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HISTOGRAMS\n",
    "\n",
    "#df.hist(figsize=(20,15),bins=100); #Note: Through this plot we noticed the first correlations through shape\n",
    "#df.groupby('Class').hist(figsize=(20,15),bins=100); #Note: It is not informative for our purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CORRELATION\n",
    "\n",
    "\"\"\" fig, ax = plt.subplots(figsize=(10,10)) \n",
    "CORR_MAP = df.corr(numeric_only = True).abs()\n",
    "Steep = 10.0\n",
    "Stretch = 2.002\n",
    "if Stretch < 2.0 :\n",
    "    Stretch = 2.0 \n",
    "HIGH_CORR = (np.tan((np.pi/Stretch) * np.abs(CORR_MAP)) * (1 / Steep)) #LOW INFORMATION\n",
    "LOW_CORR  = (np.tan((np.pi/Stretch) * (1 - np.abs(CORR_MAP))) * (1 / Steep)) #HIGH INFORMATION\n",
    "\n",
    "sns.heatmap( CORR_MAP,annot=True,linewidths=.5, cmap=\"flare\"); # cmap=\"rocket_r\"\n",
    "#sns.heatmap( HIGH_CORR,annot=True,linewidths=.5, cmap=\"flare\"); # cmap=\"rocket_r\"\n",
    "#sns.heatmap( LOW_CORR,annot=True,linewidths=.5, cmap=\"flare\"); # cmap=\"rocket_r\" \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARBITRARY DROPPING BY CORRELATION ANALYSIS\n",
    "# Note: Area and ConvexArea are almost perfectly correlated: we already noticed that mathematically one is the square root of the other.\n",
    "# Note: Compactness and Shape Factor 3 are almost perfectly correlated\n",
    "# Proposition: Erase ConvexArea and Compactness\n",
    "\n",
    "#dfb = df.drop(['ConvexArea','Compactness'], axis=1);\n",
    "#idx = 6453\n",
    "#Ed = np.sqrt(4 * df.loc[idx].Area / np.pi);\n",
    "#Co = Ed / df.loc[idx].MajorAxisLength;\n",
    "#SF3 = df.loc[idx].Area / (np.pi * (df.loc[idx].MajorAxisLength / 2) * (df.loc[idx].MajorAxisLength / 2));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRUNING BY CORRELATION\n",
    "\n",
    "\"\"\" dfc = df\n",
    "corr = dfc.corr(numeric_only = True).abs()\n",
    "drop = []\n",
    "hold = []\n",
    "p_threshold = 0.9\n",
    "\n",
    "for i in range(0,corr.shape[0]):\n",
    "    for j in range(i+1,corr.shape[1]):\n",
    "        if corr.iloc[i,j]>=p_threshold:\n",
    "            feature_i_corrsum = corr.iloc[i,:].sum()\n",
    "            feature_j_corrsum = corr.iloc[j,:].sum()\n",
    "            if feature_i_corrsum <= feature_j_corrsum:\n",
    "                #hold.append(corr.index[i])\n",
    "                #drop.append(corr.index[j])\n",
    "                hold.append(i)\n",
    "                drop.append(j)                \n",
    "            else:\n",
    "                #drop.append(corr.index[i])\n",
    "                #hold.append(corr.index[j])\n",
    "                drop.append(i)\n",
    "                hold.append(j)\n",
    "\n",
    "drop = list(set(drop))\n",
    "drop = [list(corr.index)[i] for i in drop]\n",
    "dfc = dfc.drop(drop, axis=1) \n",
    "corr = dfc.corr(numeric_only = True).abs()\n",
    "sns.heatmap( corr,annot=True,linewidths=.5, cmap=\"flare\"); # cmap=\"rocket_r\" \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RANDOM FOREST FEATURE IMPORTANCE\n",
    "\n",
    "\"\"\" dff = df\n",
    "X = dff.iloc[:,:-1]\n",
    "y = dff.iloc[:,-1:]\n",
    "X_train, X_test, y_train, y_test  = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators = 512, \n",
    "                                 criterion=\"gini\",                      # {\"gini\", \"entropy\", \"log_loss\"}  \n",
    "                                 min_samples_split = 2,\n",
    "                                 max_features = \"sqrt\",                 # {\"sqrt\", \"log2\", None}\n",
    "                                 bootstrap = True,\n",
    "                                 oob_score = False,\n",
    "                                 min_samples_leaf = 1, \n",
    "                                 max_depth = None\n",
    "                                 )\n",
    "rnd_clf.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "y_pred_rf = rnd_clf.predict(X_test)\n",
    "#print(met.accuracy_score(y_pred_rf, y_test));\n",
    "\n",
    "feat_score = rnd_clf.feature_importances_\n",
    "\n",
    "features_importances = pd.DataFrame(list(zip(features, feat_score)), columns =['features', 'score']);\n",
    "features_importances = features_importances.sort_values(by=['score'], ascending=False);\n",
    "features_importances = features_importances.reset_index(drop = True);\n",
    "No_of_best = 12\n",
    "if No_of_best < 1 or No_of_best > len(features_importances):\n",
    "    No_of_best = len(features_importances)\n",
    "best_features = features_importances.iloc[:No_of_best,0].tolist()\n",
    "best_features.append('Class')\n",
    "dfe = df.filter(best_features , axis=1);\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLASSES ANALYSIS\n",
    "\n",
    "\"\"\" fig, ax = plt.subplots(figsize=(10,5))\n",
    "graph = sns.countplot(ax=ax, x='Class', data=df, order=labels)\n",
    "graph.set_xticklabels(graph.get_xticklabels(),rotation=90)\n",
    "graph.bar_label(ax.containers[0]); \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OVERSAMPLING - SMOTE\n",
    "\n",
    "\"\"\" X = df.iloc[:,:-1]\n",
    "y = df.iloc[:,-1:]\n",
    "X_train, X_test, y_train, y_test  = train_test_split(X, y, test_size=0.2)\n",
    "smt = SMOTE()\n",
    "X_train_smt, y_train_smt = smt.fit_resample(X_train, y_train)\n",
    "df_train_smote = pd.merge(X_train_smt, y_train_smt, left_index=True, right_index=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "graph = sns.countplot(ax=ax, x='Class', data=df_train_smote, order=labels)\n",
    "graph.set_xticklabels(graph.get_xticklabels(),rotation=90)\n",
    "graph.bar_label(ax.containers[0]); \"\"\"\n",
    "\n",
    "# ATTENZIONE, IL NUMERO DI ISTANZE PER CLASSE CAMBIA CONTINUAMENTE, DOPO LO SPLIT LA CLASSE CHE NE ESCE \"VINCITRICE\",\n",
    "# LA MAGGIORMENTE SELEZIONA DETERMINA LA SOGLIA DI OVERSAMPLING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OVERSAMPLING - ADASYN\n",
    "\n",
    "\"\"\" X = df.iloc[:,:-1]\n",
    "y = df.iloc[:,-1:]\n",
    "X_train, X_test, y_train, y_test  = train_test_split(X, y, test_size=0.2)\n",
    "ada = ADASYN()\n",
    "X_train_ada, y_train_ada = ada.fit_resample(X_train, y_train)\n",
    "df_train_adasyn = pd.merge(X_train_ada, y_train_ada, left_index=True, right_index=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "graph = sns.countplot(ax=ax, x='Class', data=df_train_adasyn, order=labels)\n",
    "graph.set_xticklabels(graph.get_xticklabels(),rotation=90)\n",
    "graph.bar_label(ax.containers[0]); \"\"\"\n",
    "\n",
    "# ATTENZIONE, IL NUMERO DI ISTANZE PER CLASSE CAMBIA CONTINUAMENTE, DOPO LO SPLIT LA CLASSE CHE NE ESCE \"VINCITRICE\",\n",
    "# LA MAGGIORMENTE SELEZIONA DETERMINA LA SOGLIA DI OVERSAMPLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYBRIDIZATION - SMOTE+Tomek\n",
    "\n",
    "\"\"\" X = df.iloc[:,:-1]\n",
    "y = df.iloc[:,-1:]\n",
    "X_train, X_test, y_train, y_test  = train_test_split(X, y, test_size=0.2)\n",
    "stk = SMOTETomek()\n",
    "X_train_stk, y_train_stk = stk.fit_resample(X_train, y_train)\n",
    "df_train_stomek = pd.merge(X_train_stk, y_train_stk, left_index=True, right_index=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "graph = sns.countplot(ax=ax, x='Class', data=df_train_stomek, order=labels)\n",
    "graph.set_xticklabels(graph.get_xticklabels(),rotation=90)\n",
    "graph.bar_label(ax.containers[0]); \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYBRIDIZATION - SMOTE+ENN\n",
    "\n",
    "X = df.iloc[:,:-1]\n",
    "y = df.iloc[:,-1:]\n",
    "X_train, X_test, y_train, y_test  = train_test_split(X, y, test_size=0.2)\n",
    "snn = SMOTEENN()\n",
    "X_train_snn, y_train_snn = snn.fit_resample(X_train, y_train)\n",
    "df_train_smtenn = pd.merge(X_train_snn, y_train_snn, left_index=True, right_index=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "graph = sns.countplot(ax=ax, x='Class', data=df_train_smtenn, order=labels)\n",
    "graph.set_xticklabels(graph.get_xticklabels(),rotation=90)\n",
    "graph.bar_label(ax.containers[0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLASS + FEATURES SELECTION\n",
    "\n",
    "#drop = list(set(drop))\n",
    "#dfh = df_train_smtenn.drop(drop, axis=1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLASS BALANCING SCATTERPLOTS\n",
    "\n",
    "#dfs = [dfh] # SELECT DATAFRAME: [df, dfb, dfc, dfe, df_train_smote, df_train_adasyn, df_train_stomek, df_train_smtenn]\n",
    "\n",
    "#for item in dfs:\n",
    "#    feats = item.columns.to_list();\n",
    "#    pairs_features = [(a, b) for idx, a in enumerate(feats) for b in feats[idx + 1:]];\n",
    "#    for pair in pairs_features:\n",
    "#        plt.subplots(figsize=(10,5));\n",
    "#        for label in labels:\n",
    "#            sns.scatterplot(x=pair[0], y=pair[1], data=item[item['Class']==label]);\n",
    "#            plt.legend(labels, fontsize = 10, loc=0);\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5] END model__bootstrap=True, model__criterion=gini, model__max_features=sqrt, model__min_samples_split=2, model__n_estimators=64, model__oob_score=False;, score=(train=0.995, test=0.988) total time=   3.9s\n",
      "[CV 2/5] END model__bootstrap=True, model__criterion=gini, model__max_features=sqrt, model__min_samples_split=2, model__n_estimators=64, model__oob_score=False;, score=(train=0.996, test=0.991) total time=   3.9s\n",
      "[CV 3/5] END model__bootstrap=True, model__criterion=gini, model__max_features=sqrt, model__min_samples_split=2, model__n_estimators=64, model__oob_score=False;, score=(train=0.995, test=0.991) total time=   3.8s\n",
      "[CV 4/5] END model__bootstrap=True, model__criterion=gini, model__max_features=sqrt, model__min_samples_split=2, model__n_estimators=64, model__oob_score=False;, score=(train=0.996, test=0.988) total time=   3.8s\n",
      "[CV 5/5] END model__bootstrap=True, model__criterion=gini, model__max_features=sqrt, model__min_samples_split=2, model__n_estimators=64, model__oob_score=False;, score=(train=0.995, test=0.990) total time=   3.8s\n",
      "0.9029162052417866\n"
     ]
    }
   ],
   "source": [
    "# ENSEMBLE METHODS\n",
    "\n",
    "#dfr = df_train_smtenn # SELECT DATAFRAME: [df, dfb, dfc, dfe, df_train_smote, df_train_adasyn, df_train_stomek, df_train_smtenn]\n",
    "#X = dfr.iloc[:,:-1]\n",
    "#y = dfr.iloc[:,-1:]\n",
    "#X_train, X_test, y_train, y_test  = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "\n",
    "steps = [('over', SMOTEENN()), ('model',RandomForestClassifier())]\n",
    "pipeline = Pipeline(steps=steps)\n",
    "\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(\n",
    "        min_samples_leaf=1, max_depth = None\n",
    "    ),\n",
    "#     \"Extra Trees\": ExtraTreesClassifier(\n",
    "#        min_samples_leaf=1, max_depth = None\n",
    "#    ),\n",
    "}\n",
    "param_grids = {\n",
    "    \"Random Forest\": {\"model__n_estimators\": [64], \n",
    "                      \"model__criterion\": [\"gini\"],                    # {\"gini\", \"entropy\", \"log_loss\"}\n",
    "                      \"model__min_samples_split\": [2],\n",
    "                      \"model__max_features\": [\"sqrt\"],                 # {\"sqrt\", \"log2\", None}\n",
    "                      \"model__bootstrap\": [True],\n",
    "                      \"model__oob_score\": [False]\n",
    "                      },\n",
    "#    \"Extra Trees\":   {\"n_estimators\": [64], \n",
    "#                      \"criterion\": [\"gini\"],                    # {\"gini\", \"entropy\", \"log_loss\"}\n",
    "#                      \"min_samples_split\": [2],\n",
    "#                      \"max_features\": [\"sqrt\"],                 # {\"sqrt\", \"log2\", None}\n",
    "#                      \"bootstrap\": [False],\n",
    "#                      \"oob_score\": [False]\n",
    "#                      } \n",
    "}\n",
    "\n",
    "results = []\n",
    "#for name, model in models.items():\n",
    "grid_search = GridSearchCV(\n",
    "estimator=pipeline,\n",
    "param_grid=param_grids[\"Random Forest\"],\n",
    "#param_grid=param_grids[name],\n",
    "scoring = 'accuracy',\n",
    "n_jobs = None,\n",
    "refit = True,        \n",
    "cv=KFold(n_splits=5, shuffle=True),        \n",
    "verbose = 4,\n",
    "return_train_score=True,\n",
    ").fit(X_train_snn, y_train_snn.values.ravel())\n",
    "result = {\"model\": \"Random Forest\", \"cv_results\": pd.DataFrame(grid_search.cv_results_)}\n",
    "results.append(result)\n",
    "\n",
    "y_pred_test = grid_search.predict(X_test) #best_estimator_.\n",
    "print(met.accuracy_score(y_pred_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SCORING\n",
    "\n",
    "# CLASSES: ['BARBUNYA', 'BOMBAY', 'CALI', 'DERMASON', 'HOROZ', 'SEKER', 'SIRA']\n",
    "\n",
    "y_score = grid_search.predict_proba(X_test)\n",
    "label_binarizer = LabelBinarizer().fit(y_train)\n",
    "y_onehot_test = label_binarizer.transform(y_test)\n",
    "y_onehot_test.shape  # (n_samples, n_classes)\n",
    "label_binarizer.transform([\"SIRA\"])\n",
    "\n",
    "class_of_interest = \"SIRA\"\n",
    "class_id = np.flatnonzero(label_binarizer.classes_ == class_of_interest)[0]\n",
    "class_id\n",
    "\n",
    "met.RocCurveDisplay.from_predictions(\n",
    "    y_onehot_test[:, class_id],\n",
    "    y_score[:, class_id],\n",
    "    name=f\"{class_of_interest} vs the rest\",\n",
    "    color=\"darkorange\",\n",
    "    #plot_chance_level=True,\n",
    ")\n",
    "plt.axis(\"square\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"One-vs-Rest ROC curves: \" + class_of_interest + \" vs rest\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# FPR = 1 when threshold = 0% or TN = 0\n",
    "# TPR = 0 when threshold = 1000% or TP = 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
