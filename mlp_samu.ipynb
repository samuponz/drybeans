{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIBRARIES\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import sklearn.metrics as met\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 13543 entries, 0 to 13542\n",
      "Data columns (total 17 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   Area             13543 non-null  float64\n",
      " 1   Perimeter        13543 non-null  float64\n",
      " 2   MajorAxisLength  13543 non-null  float64\n",
      " 3   MinorAxisLength  13543 non-null  float64\n",
      " 4   AspectRatio      13543 non-null  float64\n",
      " 5   Eccentricity     13543 non-null  float64\n",
      " 6   ConvexArea       13543 non-null  int64  \n",
      " 7   EquivDiameter    13543 non-null  float64\n",
      " 8   Extent           13543 non-null  float64\n",
      " 9   Solidity         13543 non-null  float64\n",
      " 10  roundness        13543 non-null  float64\n",
      " 11  Compactness      13543 non-null  float64\n",
      " 12  ShapeFactor1     13543 non-null  float64\n",
      " 13  ShapeFactor2     13543 non-null  float64\n",
      " 14  ShapeFactor3     13543 non-null  float64\n",
      " 15  ShapeFactor4     13543 non-null  float64\n",
      " 16  Class            13543 non-null  string \n",
      "dtypes: float64(15), int64(1), string(1)\n",
      "memory usage: 1.8 MB\n"
     ]
    }
   ],
   "source": [
    "# DATASET\n",
    "\n",
    "df = p.read_excel(\"DryBeanDataset/Dry_Bean_Dataset.xlsx\")\n",
    "df = df.drop_duplicates(ignore_index=True)\n",
    "df.rename(columns = {'AspectRation':'AspectRatio'}, inplace = True)\n",
    "df = df.astype({'Area': 'float64'})\n",
    "df = df.astype({'Class': 'string'})\n",
    "df.info()\n",
    "df.describe() # df[df['Class']=='BOMBAY'].describe()\n",
    "labels = ['BARBUNYA', 'BOMBAY', 'CALI', 'DERMASON', 'HOROZ', 'SEKER', 'SIRA']\n",
    "colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple', 'tab:brown', 'tab:pink']\n",
    "features = df.columns.to_list();\n",
    "features.remove('Class');\n",
    "#features = ['Area', 'Perimeter', 'MajorAxisLength', 'MinorAxisLength', 'AspectRatio', 'Eccentricity', 'ConvexArea', 'EquivDiameter', \n",
    "#            'Extent', 'Solidity', 'roundness', 'Compactness', 'ShapeFactor1', 'ShapeFactor2','ShapeFactor3', 'ShapeFactor4'];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN TEST SPLIT\n",
    "\n",
    "X = df.iloc[:,:-1]\n",
    "y = df.iloc[:,-1:]\n",
    "X_train, X_test, y_train, y_test  = train_test_split(X, y, test_size=0.33)\n",
    "df_train = pd.merge(X_train, y_train, left_index=True, right_index=True)\n",
    "df_test = pd.merge(X_test, y_test, left_index=True, right_index=True)\n",
    "\n",
    "# Feature Scaling \n",
    "sc = MinMaxScaler()\n",
    "X_train_scaled = sc.fit_transform(X_train)\n",
    "X_test_scaled = sc.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.98058140\n",
      "Iteration 2, loss = 1.89634624\n",
      "Iteration 3, loss = 1.85457313\n",
      "Iteration 4, loss = 1.82857516\n",
      "Iteration 5, loss = 1.81365801\n",
      "Iteration 6, loss = 1.80173151\n",
      "Iteration 7, loss = 1.79107762\n",
      "Iteration 8, loss = 1.78077539\n",
      "Iteration 9, loss = 1.76991734\n",
      "Iteration 10, loss = 1.75810164\n",
      "Iteration 11, loss = 1.74529263\n",
      "Iteration 12, loss = 1.73100564\n",
      "Iteration 13, loss = 1.71500112\n",
      "Iteration 14, loss = 1.69688368\n",
      "Iteration 15, loss = 1.67646115\n",
      "Iteration 16, loss = 1.65346624\n",
      "Iteration 17, loss = 1.62804120\n",
      "Iteration 18, loss = 1.60066644\n",
      "Iteration 19, loss = 1.57151090\n",
      "Iteration 20, loss = 1.53981016\n",
      "Iteration 21, loss = 1.50224888\n",
      "Iteration 22, loss = 1.45545796\n",
      "Iteration 23, loss = 1.41225558\n",
      "Iteration 24, loss = 1.37022636\n",
      "Iteration 25, loss = 1.32771713\n",
      "Iteration 26, loss = 1.28751868\n",
      "Iteration 27, loss = 1.24960766\n",
      "Iteration 28, loss = 1.21323889\n",
      "Iteration 29, loss = 1.17741960\n",
      "Iteration 30, loss = 1.14172950\n",
      "Iteration 31, loss = 1.10613752\n",
      "Iteration 32, loss = 1.06903411\n",
      "Iteration 33, loss = 1.02990908\n",
      "Iteration 34, loss = 0.99109334\n",
      "Iteration 35, loss = 0.95364511\n",
      "Iteration 36, loss = 0.91540313\n",
      "Iteration 37, loss = 0.87617426\n",
      "Iteration 38, loss = 0.83515031\n",
      "Iteration 39, loss = 0.79277618\n",
      "Iteration 40, loss = 0.74901638\n",
      "Iteration 41, loss = 0.70500103\n",
      "Iteration 42, loss = 0.66203559\n",
      "Iteration 43, loss = 0.62020133\n",
      "Iteration 44, loss = 0.58160684\n",
      "Iteration 45, loss = 0.54576346\n",
      "Iteration 46, loss = 0.51389951\n",
      "Iteration 47, loss = 0.48579683\n",
      "Iteration 48, loss = 0.46151147\n",
      "Iteration 49, loss = 0.44019453\n",
      "Iteration 50, loss = 0.42185756\n",
      "Iteration 51, loss = 0.40606748\n",
      "Iteration 52, loss = 0.39215914\n",
      "Iteration 53, loss = 0.38054320\n",
      "Iteration 54, loss = 0.37024449\n",
      "Iteration 55, loss = 0.36155134\n",
      "Iteration 56, loss = 0.35331925\n",
      "Iteration 57, loss = 0.34656522\n",
      "Iteration 58, loss = 0.34082302\n",
      "Iteration 59, loss = 0.33488000\n",
      "Iteration 60, loss = 0.33064835\n",
      "Iteration 61, loss = 0.32589218\n",
      "Iteration 62, loss = 0.32189779\n",
      "Iteration 63, loss = 0.31862250\n",
      "Iteration 64, loss = 0.31531953\n",
      "Iteration 65, loss = 0.31249940\n",
      "Iteration 66, loss = 0.30956022\n",
      "Iteration 67, loss = 0.30737025\n",
      "Iteration 68, loss = 0.30520519\n",
      "Iteration 69, loss = 0.30305720\n",
      "Iteration 70, loss = 0.30144126\n",
      "Iteration 71, loss = 0.29915317\n",
      "Iteration 72, loss = 0.29758227\n",
      "Iteration 73, loss = 0.29644483\n",
      "Iteration 74, loss = 0.29505157\n",
      "Iteration 75, loss = 0.29314188\n",
      "Iteration 76, loss = 0.29224482\n",
      "Iteration 77, loss = 0.29085421\n",
      "Iteration 78, loss = 0.28956872\n",
      "Iteration 79, loss = 0.28856899\n",
      "Iteration 80, loss = 0.28752997\n",
      "Iteration 81, loss = 0.28671093\n",
      "Iteration 82, loss = 0.28549764\n",
      "Iteration 83, loss = 0.28464117\n",
      "Iteration 84, loss = 0.28385928\n",
      "Iteration 85, loss = 0.28289218\n",
      "Iteration 86, loss = 0.28284198\n",
      "Iteration 87, loss = 0.28160374\n",
      "Iteration 88, loss = 0.28087178\n",
      "Iteration 89, loss = 0.28091967\n",
      "Iteration 90, loss = 0.27923779\n",
      "Iteration 91, loss = 0.27852580\n",
      "Iteration 92, loss = 0.27799565\n",
      "Iteration 93, loss = 0.27707343\n",
      "Iteration 94, loss = 0.27687387\n",
      "Iteration 95, loss = 0.27643268\n",
      "Iteration 96, loss = 0.27562575\n",
      "Iteration 97, loss = 0.27534923\n",
      "Iteration 98, loss = 0.27503289\n",
      "Iteration 99, loss = 0.27353378\n",
      "Iteration 100, loss = 0.27388874\n",
      "Iteration 101, loss = 0.27264566\n",
      "Iteration 102, loss = 0.27222194\n",
      "Iteration 103, loss = 0.27184750\n",
      "Iteration 104, loss = 0.27145102\n",
      "Iteration 105, loss = 0.27113884\n",
      "Iteration 106, loss = 0.27070368\n",
      "Iteration 107, loss = 0.27064625\n",
      "Iteration 108, loss = 0.27023116\n",
      "Iteration 109, loss = 0.26934825\n",
      "Iteration 110, loss = 0.26918518\n",
      "Iteration 111, loss = 0.26890609\n",
      "Iteration 112, loss = 0.26805667\n",
      "Iteration 113, loss = 0.26778654\n",
      "Iteration 114, loss = 0.26735794\n",
      "Iteration 115, loss = 0.26729302\n",
      "Iteration 116, loss = 0.26700136\n",
      "Iteration 117, loss = 0.26653899\n",
      "Iteration 118, loss = 0.26613860\n",
      "Iteration 119, loss = 0.26588768\n",
      "Iteration 120, loss = 0.26507435\n",
      "Iteration 121, loss = 0.26529933\n",
      "Iteration 122, loss = 0.26651867\n",
      "Iteration 123, loss = 0.26426702\n",
      "Iteration 124, loss = 0.26503435\n",
      "Iteration 125, loss = 0.26414960\n",
      "Iteration 126, loss = 0.26363196\n",
      "Iteration 127, loss = 0.26363895\n",
      "Iteration 128, loss = 0.26278368\n",
      "Iteration 129, loss = 0.26280224\n",
      "Iteration 130, loss = 0.26262274\n",
      "Iteration 131, loss = 0.26210813\n",
      "Iteration 132, loss = 0.26219011\n",
      "Iteration 133, loss = 0.26242157\n",
      "Iteration 134, loss = 0.26159978\n",
      "Iteration 135, loss = 0.26133043\n",
      "Iteration 136, loss = 0.26114381\n",
      "Iteration 137, loss = 0.26068468\n",
      "Iteration 138, loss = 0.26088645\n",
      "Iteration 139, loss = 0.26051307\n",
      "Iteration 140, loss = 0.25995959\n",
      "Iteration 141, loss = 0.25979296\n",
      "Iteration 142, loss = 0.26050377\n",
      "Iteration 143, loss = 0.25986100\n",
      "Iteration 144, loss = 0.25912512\n",
      "Iteration 145, loss = 0.25920410\n",
      "Iteration 146, loss = 0.25970210\n",
      "Iteration 147, loss = 0.25859646\n",
      "Iteration 148, loss = 0.25833138\n",
      "Iteration 149, loss = 0.25894009\n",
      "Iteration 150, loss = 0.25833975\n",
      "Iteration 151, loss = 0.25768097\n",
      "Iteration 152, loss = 0.25844829\n",
      "Iteration 153, loss = 0.25818111\n",
      "Iteration 154, loss = 0.25731401\n",
      "Iteration 155, loss = 0.25719922\n",
      "Iteration 156, loss = 0.25763171\n",
      "Iteration 157, loss = 0.25739236\n",
      "Iteration 158, loss = 0.25678629\n",
      "Iteration 159, loss = 0.25664701\n",
      "Iteration 160, loss = 0.25642373\n",
      "Iteration 161, loss = 0.25660085\n",
      "Iteration 162, loss = 0.25675437\n",
      "Iteration 163, loss = 0.25616523\n",
      "Iteration 164, loss = 0.25563791\n",
      "Iteration 165, loss = 0.25569001\n",
      "Iteration 166, loss = 0.25536093\n",
      "Iteration 167, loss = 0.25556753\n",
      "Iteration 168, loss = 0.25557846\n",
      "Iteration 169, loss = 0.25500870\n",
      "Iteration 170, loss = 0.25488940\n",
      "Iteration 171, loss = 0.25472312\n",
      "Iteration 172, loss = 0.25498145\n",
      "Iteration 173, loss = 0.25490288\n",
      "Iteration 174, loss = 0.25439278\n",
      "Iteration 175, loss = 0.25457266\n",
      "Iteration 176, loss = 0.25418916\n",
      "Iteration 177, loss = 0.25399364\n",
      "Iteration 178, loss = 0.25443573\n",
      "Iteration 179, loss = 0.25370277\n",
      "Iteration 180, loss = 0.25375374\n",
      "Iteration 181, loss = 0.25359860\n",
      "Iteration 182, loss = 0.25343164\n",
      "Iteration 183, loss = 0.25284708\n",
      "Iteration 184, loss = 0.25289055\n",
      "Iteration 185, loss = 0.25329137\n",
      "Iteration 186, loss = 0.25368074\n",
      "Iteration 187, loss = 0.25320621\n",
      "Iteration 188, loss = 0.25226358\n",
      "Iteration 189, loss = 0.25267522\n",
      "Iteration 190, loss = 0.25283222\n",
      "Iteration 191, loss = 0.25230711\n",
      "Iteration 192, loss = 0.25178512\n",
      "Iteration 193, loss = 0.25247164\n",
      "Iteration 194, loss = 0.25172011\n",
      "Iteration 195, loss = 0.25135634\n",
      "Iteration 196, loss = 0.25198479\n",
      "Iteration 197, loss = 0.25276704\n",
      "Iteration 198, loss = 0.25136914\n",
      "Iteration 199, loss = 0.25111600\n",
      "Iteration 200, loss = 0.25150549\n",
      "Iteration 201, loss = 0.25124904\n",
      "Iteration 202, loss = 0.25100343\n",
      "Iteration 203, loss = 0.25130971\n",
      "Iteration 204, loss = 0.25035128\n",
      "Iteration 205, loss = 0.25108044\n",
      "Iteration 206, loss = 0.25042011\n",
      "Iteration 207, loss = 0.25140587\n",
      "Iteration 208, loss = 0.25087192\n",
      "Iteration 209, loss = 0.25061822\n",
      "Iteration 210, loss = 0.25056619\n",
      "Iteration 211, loss = 0.25056598\n",
      "Iteration 212, loss = 0.25006472\n",
      "Iteration 213, loss = 0.25020116\n",
      "Iteration 214, loss = 0.24952371\n",
      "Iteration 215, loss = 0.25011262\n",
      "Iteration 216, loss = 0.25033168\n",
      "Iteration 217, loss = 0.25054443\n",
      "Iteration 218, loss = 0.24950258\n",
      "Iteration 219, loss = 0.24854266\n",
      "Iteration 220, loss = 0.24945876\n",
      "Iteration 221, loss = 0.24947920\n",
      "Iteration 222, loss = 0.24977721\n",
      "Iteration 223, loss = 0.24867396\n",
      "Iteration 224, loss = 0.24881056\n",
      "Iteration 225, loss = 0.24830950\n",
      "Iteration 226, loss = 0.24866444\n",
      "Iteration 227, loss = 0.24873423\n",
      "Iteration 228, loss = 0.24817788\n",
      "Iteration 229, loss = 0.24872837\n",
      "Iteration 230, loss = 0.24844599\n",
      "Iteration 231, loss = 0.24804411\n",
      "Iteration 232, loss = 0.24846777\n",
      "Iteration 233, loss = 0.24847343\n",
      "Iteration 234, loss = 0.24835713\n",
      "Iteration 235, loss = 0.24796731\n",
      "Iteration 236, loss = 0.24861303\n",
      "Iteration 237, loss = 0.24795841\n",
      "Iteration 238, loss = 0.24776005\n",
      "Iteration 239, loss = 0.24817224\n",
      "Iteration 240, loss = 0.24788791\n",
      "Iteration 241, loss = 0.24783550\n",
      "Iteration 242, loss = 0.24737019\n",
      "Iteration 243, loss = 0.24738991\n",
      "Iteration 244, loss = 0.24768998\n",
      "Iteration 245, loss = 0.24774821\n",
      "Iteration 246, loss = 0.24697205\n",
      "Iteration 247, loss = 0.24730156\n",
      "Iteration 248, loss = 0.24707078\n",
      "Iteration 249, loss = 0.24656679\n",
      "Iteration 250, loss = 0.24820993\n",
      "Iteration 251, loss = 0.24639266\n",
      "Iteration 252, loss = 0.24630982\n",
      "Iteration 253, loss = 0.24626498\n",
      "Iteration 254, loss = 0.24622961\n",
      "Iteration 255, loss = 0.24630228\n",
      "Iteration 256, loss = 0.24639971\n",
      "Iteration 257, loss = 0.24615752\n",
      "Iteration 258, loss = 0.24627442\n",
      "Iteration 259, loss = 0.24678868\n",
      "Iteration 260, loss = 0.24601229\n",
      "Iteration 261, loss = 0.24643135\n",
      "Iteration 262, loss = 0.24607418\n",
      "Iteration 263, loss = 0.24624678\n",
      "Iteration 264, loss = 0.24604537\n",
      "Iteration 265, loss = 0.24619355\n",
      "Iteration 266, loss = 0.24596163\n",
      "Iteration 267, loss = 0.24664331\n",
      "Iteration 268, loss = 0.24571964\n",
      "Iteration 269, loss = 0.24509319\n",
      "Iteration 270, loss = 0.24503201\n",
      "Iteration 271, loss = 0.24553452\n",
      "Iteration 272, loss = 0.24486608\n",
      "Iteration 273, loss = 0.24459077\n",
      "Iteration 274, loss = 0.24526416\n",
      "Iteration 275, loss = 0.24462549\n",
      "Iteration 276, loss = 0.24463911\n",
      "Iteration 277, loss = 0.24495920\n",
      "Iteration 278, loss = 0.24407736\n",
      "Iteration 279, loss = 0.24468956\n",
      "Iteration 280, loss = 0.24447515\n",
      "Iteration 281, loss = 0.24428401\n",
      "Iteration 282, loss = 0.24406724\n",
      "Iteration 283, loss = 0.24429400\n",
      "Iteration 284, loss = 0.24458446\n",
      "Iteration 285, loss = 0.24424607\n",
      "Iteration 286, loss = 0.24374555\n",
      "Iteration 287, loss = 0.24427675\n",
      "Iteration 288, loss = 0.24386236\n",
      "Iteration 289, loss = 0.24388429\n",
      "Iteration 290, loss = 0.24320287\n",
      "Iteration 291, loss = 0.24419038\n",
      "Iteration 292, loss = 0.24350148\n",
      "Iteration 293, loss = 0.24333681\n",
      "Iteration 294, loss = 0.24306860\n",
      "Iteration 295, loss = 0.24278879\n",
      "Iteration 296, loss = 0.24280601\n",
      "Iteration 297, loss = 0.24277203\n",
      "Iteration 298, loss = 0.24436381\n",
      "Iteration 299, loss = 0.24275024\n",
      "Iteration 300, loss = 0.24290358\n",
      "Iteration 301, loss = 0.24285218\n",
      "Iteration 302, loss = 0.24268973\n",
      "Iteration 303, loss = 0.24202798\n",
      "Iteration 304, loss = 0.24261253\n",
      "Iteration 305, loss = 0.24217979\n",
      "Iteration 306, loss = 0.24291654\n",
      "Iteration 307, loss = 0.24226643\n",
      "Iteration 308, loss = 0.24197010\n",
      "Iteration 309, loss = 0.24163823\n",
      "Iteration 310, loss = 0.24228215\n",
      "Iteration 311, loss = 0.24139299\n",
      "Iteration 312, loss = 0.24190785\n",
      "Iteration 313, loss = 0.24152775\n",
      "Iteration 314, loss = 0.24184041\n",
      "Iteration 315, loss = 0.24133056\n",
      "Iteration 316, loss = 0.24074609\n",
      "Iteration 317, loss = 0.24117554\n",
      "Iteration 318, loss = 0.24093295\n",
      "Iteration 319, loss = 0.24116844\n",
      "Iteration 320, loss = 0.24109850\n",
      "Iteration 321, loss = 0.23992969\n",
      "Iteration 322, loss = 0.24045523\n",
      "Iteration 323, loss = 0.24062201\n",
      "Iteration 324, loss = 0.24020163\n",
      "Iteration 325, loss = 0.24022753\n",
      "Iteration 326, loss = 0.23989021\n",
      "Iteration 327, loss = 0.23963227\n",
      "Iteration 328, loss = 0.24051795\n",
      "Iteration 329, loss = 0.23997165\n",
      "Iteration 330, loss = 0.24015781\n",
      "Iteration 331, loss = 0.23959907\n",
      "Iteration 332, loss = 0.23969487\n",
      "Iteration 333, loss = 0.23898899\n",
      "Iteration 334, loss = 0.23973594\n",
      "Iteration 335, loss = 0.23987712\n",
      "Iteration 336, loss = 0.23886678\n",
      "Iteration 337, loss = 0.23908568\n",
      "Iteration 338, loss = 0.23845596\n",
      "Iteration 339, loss = 0.23907349\n",
      "Iteration 340, loss = 0.23873269\n",
      "Iteration 341, loss = 0.23949288\n",
      "Iteration 342, loss = 0.23878733\n",
      "Iteration 343, loss = 0.23806845\n",
      "Iteration 344, loss = 0.23790321\n",
      "Iteration 345, loss = 0.23802626\n",
      "Iteration 346, loss = 0.23870355\n",
      "Iteration 347, loss = 0.23806260\n",
      "Iteration 348, loss = 0.23754783\n",
      "Iteration 349, loss = 0.23798586\n",
      "Iteration 350, loss = 0.23742938\n",
      "Iteration 351, loss = 0.23743814\n",
      "Iteration 352, loss = 0.23714334\n",
      "Iteration 353, loss = 0.23724044\n",
      "Iteration 354, loss = 0.23731997\n",
      "Iteration 355, loss = 0.23710742\n",
      "Iteration 356, loss = 0.23657033\n",
      "Iteration 357, loss = 0.23688929\n",
      "Iteration 358, loss = 0.23728541\n",
      "Iteration 359, loss = 0.23656360\n",
      "Iteration 360, loss = 0.23702615\n",
      "Iteration 361, loss = 0.23581071\n",
      "Iteration 362, loss = 0.23598518\n",
      "Iteration 363, loss = 0.23618292\n",
      "Iteration 364, loss = 0.23580120\n",
      "Iteration 365, loss = 0.23591463\n",
      "Iteration 366, loss = 0.23661025\n",
      "Iteration 367, loss = 0.23569832\n",
      "Iteration 368, loss = 0.23509218\n",
      "Iteration 369, loss = 0.23520847\n",
      "Iteration 370, loss = 0.23593517\n",
      "Iteration 371, loss = 0.23532127\n",
      "Iteration 372, loss = 0.23620539\n",
      "Iteration 373, loss = 0.23449029\n",
      "Iteration 374, loss = 0.23531626\n",
      "Iteration 375, loss = 0.23482005\n",
      "Iteration 376, loss = 0.23483128\n",
      "Iteration 377, loss = 0.23432674\n",
      "Iteration 378, loss = 0.23433023\n",
      "Iteration 379, loss = 0.23454139\n",
      "Iteration 380, loss = 0.23502178\n",
      "Iteration 381, loss = 0.23415349\n",
      "Iteration 382, loss = 0.23451380\n",
      "Iteration 383, loss = 0.23358538\n",
      "Iteration 384, loss = 0.23358951\n",
      "Iteration 385, loss = 0.23389747\n",
      "Iteration 386, loss = 0.23398298\n",
      "Iteration 387, loss = 0.23289815\n",
      "Iteration 388, loss = 0.23273288\n",
      "Iteration 389, loss = 0.23334055\n",
      "Iteration 390, loss = 0.23262387\n",
      "Iteration 391, loss = 0.23362981\n",
      "Iteration 392, loss = 0.23333205\n",
      "Iteration 393, loss = 0.23260612\n",
      "Iteration 394, loss = 0.23234786\n",
      "Iteration 395, loss = 0.23259347\n",
      "Iteration 396, loss = 0.23246464\n",
      "Iteration 397, loss = 0.23275669\n",
      "Iteration 398, loss = 0.23215800\n",
      "Iteration 399, loss = 0.23185475\n",
      "Iteration 400, loss = 0.23165778\n",
      "Iteration 401, loss = 0.23180182\n",
      "Iteration 402, loss = 0.23168440\n",
      "Iteration 403, loss = 0.23096821\n",
      "Iteration 404, loss = 0.23160579\n",
      "Iteration 405, loss = 0.23110690\n",
      "Iteration 406, loss = 0.23157457\n",
      "Iteration 407, loss = 0.23077091\n",
      "Iteration 408, loss = 0.23128292\n",
      "Iteration 409, loss = 0.23023156\n",
      "Iteration 410, loss = 0.23089309\n",
      "Iteration 411, loss = 0.23041918\n",
      "Iteration 412, loss = 0.23067801\n",
      "Iteration 413, loss = 0.22970040\n",
      "Iteration 414, loss = 0.23073468\n",
      "Iteration 415, loss = 0.23017518\n",
      "Iteration 416, loss = 0.23027771\n",
      "Iteration 417, loss = 0.22984896\n",
      "Iteration 418, loss = 0.22948659\n",
      "Iteration 419, loss = 0.22927174\n",
      "Iteration 420, loss = 0.22870879\n",
      "Iteration 421, loss = 0.22959129\n",
      "Iteration 422, loss = 0.23009501\n",
      "Iteration 423, loss = 0.22875740\n",
      "Iteration 424, loss = 0.22846684\n",
      "Iteration 425, loss = 0.22867100\n",
      "Iteration 426, loss = 0.22913195\n",
      "Iteration 427, loss = 0.22823978\n",
      "Iteration 428, loss = 0.22826344\n",
      "Iteration 429, loss = 0.22798412\n",
      "Iteration 430, loss = 0.22760020\n",
      "Iteration 431, loss = 0.22750385\n",
      "Iteration 432, loss = 0.22747862\n",
      "Iteration 433, loss = 0.22739042\n",
      "Iteration 434, loss = 0.22733188\n",
      "Iteration 435, loss = 0.22700168\n",
      "Iteration 436, loss = 0.22785304\n",
      "Iteration 437, loss = 0.22654831\n",
      "Iteration 438, loss = 0.22726332\n",
      "Iteration 439, loss = 0.22761324\n",
      "Iteration 440, loss = 0.22654810\n",
      "Iteration 441, loss = 0.22721698\n",
      "Iteration 442, loss = 0.22671596\n",
      "Iteration 443, loss = 0.22647498\n",
      "Iteration 444, loss = 0.22609780\n",
      "Iteration 445, loss = 0.22586614\n",
      "Iteration 446, loss = 0.22670420\n",
      "Iteration 447, loss = 0.22634974\n",
      "Iteration 448, loss = 0.22556113\n",
      "Iteration 449, loss = 0.22544116\n",
      "Iteration 450, loss = 0.22543622\n",
      "Iteration 451, loss = 0.22632115\n",
      "Iteration 452, loss = 0.22543286\n",
      "Iteration 453, loss = 0.22512283\n",
      "Iteration 454, loss = 0.22454324\n",
      "Iteration 455, loss = 0.22521762\n",
      "Iteration 456, loss = 0.22474869\n",
      "Iteration 457, loss = 0.22483142\n",
      "Iteration 458, loss = 0.22499633\n",
      "Iteration 459, loss = 0.22448485\n",
      "Iteration 460, loss = 0.22418723\n",
      "Iteration 461, loss = 0.22499550\n",
      "Iteration 462, loss = 0.22373556\n",
      "Iteration 463, loss = 0.22479341\n",
      "Iteration 464, loss = 0.22391749\n",
      "Iteration 465, loss = 0.22393072\n",
      "Iteration 466, loss = 0.22406954\n",
      "Iteration 467, loss = 0.22435072\n",
      "Iteration 468, loss = 0.22353558\n",
      "Iteration 469, loss = 0.22372906\n",
      "Iteration 470, loss = 0.22331041\n",
      "Iteration 471, loss = 0.22322341\n",
      "Iteration 472, loss = 0.22281979\n",
      "Iteration 473, loss = 0.22280614\n",
      "Iteration 474, loss = 0.22333717\n",
      "Iteration 475, loss = 0.22257619\n",
      "Iteration 476, loss = 0.22304550\n",
      "Iteration 477, loss = 0.22290257\n",
      "Iteration 478, loss = 0.22220126\n",
      "Iteration 479, loss = 0.22306918\n",
      "Iteration 480, loss = 0.22208897\n",
      "Iteration 481, loss = 0.22323378\n",
      "Iteration 482, loss = 0.22230066\n",
      "Iteration 483, loss = 0.22202723\n",
      "Iteration 484, loss = 0.22171817\n",
      "Iteration 485, loss = 0.22182659\n",
      "Iteration 486, loss = 0.22263656\n",
      "Iteration 487, loss = 0.22177985\n",
      "Iteration 488, loss = 0.22167358\n",
      "Iteration 489, loss = 0.22105612\n",
      "Iteration 490, loss = 0.22183271\n",
      "Iteration 491, loss = 0.22206344\n",
      "Iteration 492, loss = 0.22096062\n",
      "Iteration 493, loss = 0.22233187\n",
      "Iteration 494, loss = 0.22194025\n",
      "Iteration 495, loss = 0.22141060\n",
      "Iteration 496, loss = 0.22128134\n",
      "Iteration 497, loss = 0.22142625\n",
      "Iteration 498, loss = 0.22091852\n",
      "Iteration 499, loss = 0.22141804\n",
      "Iteration 500, loss = 0.22025699\n",
      "Iteration 501, loss = 0.22039793\n",
      "Iteration 502, loss = 0.22048186\n",
      "Iteration 503, loss = 0.22002381\n",
      "Iteration 504, loss = 0.22148332\n",
      "Iteration 505, loss = 0.22058279\n",
      "Iteration 506, loss = 0.21987999\n",
      "Iteration 507, loss = 0.21954051\n",
      "Iteration 508, loss = 0.21998495\n",
      "Iteration 509, loss = 0.22000295\n",
      "Iteration 510, loss = 0.21982634\n",
      "Iteration 511, loss = 0.21918937\n",
      "Iteration 512, loss = 0.21941030\n",
      "Iteration 513, loss = 0.21958938\n",
      "Iteration 514, loss = 0.21959196\n",
      "Iteration 515, loss = 0.21969155\n",
      "Iteration 516, loss = 0.21931109\n",
      "Iteration 517, loss = 0.21961184\n",
      "Iteration 518, loss = 0.21909637\n",
      "Iteration 519, loss = 0.21946093\n",
      "Iteration 520, loss = 0.21914858\n",
      "Iteration 521, loss = 0.21873036\n",
      "Iteration 522, loss = 0.21946227\n",
      "Iteration 523, loss = 0.21850489\n",
      "Iteration 524, loss = 0.21849095\n",
      "Iteration 525, loss = 0.21906116\n",
      "Iteration 526, loss = 0.21877140\n",
      "Iteration 527, loss = 0.21834328\n",
      "Iteration 528, loss = 0.21846482\n",
      "Iteration 529, loss = 0.21866902\n",
      "Iteration 530, loss = 0.21815824\n",
      "Iteration 531, loss = 0.21869709\n",
      "Iteration 532, loss = 0.21795089\n",
      "Iteration 533, loss = 0.21884493\n",
      "Iteration 534, loss = 0.21807693\n",
      "Iteration 535, loss = 0.21841616\n",
      "Iteration 536, loss = 0.21916860\n",
      "Iteration 537, loss = 0.21776200\n",
      "Iteration 538, loss = 0.21816121\n",
      "Iteration 539, loss = 0.21805995\n",
      "Iteration 540, loss = 0.21751361\n",
      "Iteration 541, loss = 0.21781640\n",
      "Iteration 542, loss = 0.21745518\n",
      "Iteration 543, loss = 0.21795781\n",
      "Iteration 544, loss = 0.21864650\n",
      "Iteration 545, loss = 0.21749145\n",
      "Iteration 546, loss = 0.21724794\n",
      "Iteration 547, loss = 0.21751174\n",
      "Iteration 548, loss = 0.21779828\n",
      "Iteration 549, loss = 0.21723829\n",
      "Iteration 550, loss = 0.21702959\n",
      "Iteration 551, loss = 0.21695741\n",
      "Iteration 552, loss = 0.21891862\n",
      "Iteration 553, loss = 0.21732341\n",
      "Iteration 554, loss = 0.21723662\n",
      "Iteration 555, loss = 0.21731363\n",
      "Iteration 556, loss = 0.21667614\n",
      "Iteration 557, loss = 0.21741227\n",
      "Iteration 558, loss = 0.21662309\n",
      "Iteration 559, loss = 0.21662483\n",
      "Iteration 560, loss = 0.21718336\n",
      "Iteration 561, loss = 0.21709736\n",
      "Iteration 562, loss = 0.21701537\n",
      "Iteration 563, loss = 0.21678045\n",
      "Iteration 564, loss = 0.21661585\n",
      "Iteration 565, loss = 0.21623891\n",
      "Iteration 566, loss = 0.21660904\n",
      "Iteration 567, loss = 0.21631855\n",
      "Iteration 568, loss = 0.21659495\n",
      "Iteration 569, loss = 0.21612635\n",
      "Iteration 570, loss = 0.21638340\n",
      "Iteration 571, loss = 0.21590781\n",
      "Iteration 572, loss = 0.21575720\n",
      "Iteration 573, loss = 0.21670765\n",
      "Iteration 574, loss = 0.21619104\n",
      "Iteration 575, loss = 0.21629974\n",
      "Iteration 576, loss = 0.21584570\n",
      "Iteration 577, loss = 0.21600779\n",
      "Iteration 578, loss = 0.21510661\n",
      "Iteration 579, loss = 0.21628196\n",
      "Iteration 580, loss = 0.21588158\n",
      "Iteration 581, loss = 0.21624134\n",
      "Iteration 582, loss = 0.21527635\n",
      "Iteration 583, loss = 0.21533423\n",
      "Iteration 584, loss = 0.21548992\n",
      "Iteration 585, loss = 0.21604439\n",
      "Iteration 586, loss = 0.21596282\n",
      "Iteration 587, loss = 0.21543458\n",
      "Iteration 588, loss = 0.21545552\n",
      "Iteration 589, loss = 0.21527316\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    BARBUNYA       0.91      0.94      0.93       442\n",
      "      BOMBAY       1.00      0.99      1.00       164\n",
      "        CALI       0.95      0.94      0.95       567\n",
      "    DERMASON       0.89      0.92      0.91      1099\n",
      "       HOROZ       0.96      0.96      0.96       627\n",
      "       SEKER       0.93      0.96      0.94       641\n",
      "        SIRA       0.89      0.84      0.86       930\n",
      "\n",
      "    accuracy                           0.92      4470\n",
      "   macro avg       0.93      0.94      0.93      4470\n",
      "weighted avg       0.92      0.92      0.92      4470\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = MLPClassifier(\n",
    "    hidden_layer_sizes=(16, 16, 16), \n",
    "    activation='relu',\n",
    "    solver='sgd', \n",
    "    alpha=1e-5, \n",
    "    learning_rate='constant',\n",
    "    learning_rate_init=0.001,    \n",
    "    max_iter=1000,\n",
    "    tol=1e-4,\n",
    "    verbose=True,\n",
    "    momentum=0.9,\n",
    "    early_stopping=False,\n",
    "    )\n",
    "\n",
    "clf.fit(X_train_scaled, y_train.values.ravel())\n",
    "\n",
    "y_pred_test = clf.predict(X_test_scaled) #best_estimator_.\n",
    "print(met.classification_report(y_pred_test, y_test));\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
