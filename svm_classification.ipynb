{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "Regarding Pre-Processing, it is necessary to select:\n",
    " - the best scaling strategy, necessary pre-processing step for the SVM classifier;\n",
    " - the best feature selection/feature extraction method. Try with:\n",
    "    - PCA (feature extraction for dimensionality reduction);\n",
    "    - Filter methods such as SelectKBest, SelectPercentile. The metric to use should be the f_classif (ANOVA correlation since attributes are numerical and targets are categorical).\n",
    "- the best way to balance out the classes:\n",
    "    - Oversampling;\n",
    "    - Undersampling;\n",
    "    - built-in methods used by SVC class.\n",
    "- the best way to remove the outliers.\n",
    "    \n",
    "Regarding the classification, it is necessary to select:\n",
    " - the kernel function of the SVM (linear, polynomial, rbf, sigmoid) and the respective parameters (e.g. gamma for the rbf);\n",
    " - the regulatization parameter;\n",
    " - the multiclassification method (ovo or ova);\n",
    " \n",
    " Regarding the training procedure, it is necessary to select:\n",
    "  - the best train\\test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIBRARIES\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from pandas._libs import json\n",
    "import sklearn.decomposition as dec\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import make_scorer, accuracy_score, plot_confusion_matrix\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "import sklearn.metrics as met\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.pipeline import Pipeline \n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_balancing(X_train, y_train, method):\n",
    "    \n",
    "    if method == 'smote':\n",
    "        sampler = SMOTE()\n",
    "    elif method == 'adasyn':\n",
    "        sampler = ADASYN()\n",
    "    elif method == 'smote_tomek':\n",
    "        sampler = SMOTETomek()\n",
    "    elif method == 'smote_enn':\n",
    "        sampler = SMOTEENN()\n",
    "    \n",
    "    X_train_resampled, y_train_resampled = sampler.fit_resample(X_train, y_train)\n",
    "\n",
    "    return X_train_resampled, y_train_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_classification(df):\n",
    "\n",
    "    df_train, df_test = train_test_split(df, test_size=0.2, random_state=0)\n",
    "    df_traindata, df_trainlabel = df_train.iloc[:, 0:len(df_train.columns) - 1], df_train.iloc[:, -1]\n",
    "    df_testdata, df_testlabel = df_test.iloc[:, 0:len(df_test.columns) - 1], df_test.iloc[:, -1]\n",
    "\n",
    "    warnings.filterwarnings('ignore')\n",
    "    # Baseline - comparing model accuracy using all features across classifiers\n",
    "    classifiers = [\n",
    "        DecisionTreeClassifier(),\n",
    "        RandomForestClassifier(),\n",
    "        KNeighborsClassifier(),\n",
    "        SVC(),\n",
    "        GaussianNB(),\n",
    "        LogisticRegression(),\n",
    "        MLPClassifier()\n",
    "    ]\n",
    "\n",
    "    # Naive Train Accuracy\n",
    "    algo = ['DTC', 'RFC', 'KNN', 'SVC', 'GNB', 'LR']\n",
    "    # algo = []\n",
    "    scores = []\n",
    "    for clf in classifiers:\n",
    "        # algo.append(clf.__class__.__name__)\n",
    "        scores.append(cross_val_score(clf, df_traindata, df_trainlabel, cv=5).mean())\n",
    "    warnings.filterwarnings('ignore')\n",
    "    Naivescore_df_Train = pd.DataFrame({'Algorithm': algo, 'Score': scores}).set_index('Algorithm')\n",
    "\n",
    "    # Naive Test Accuracy\n",
    "    algo = ['DTC', 'RFC', 'KNN', 'SVC', 'GNB', 'LR']\n",
    "    # algo = []\n",
    "    scores = []\n",
    "\n",
    "    for clf in classifiers:\n",
    "        clf = clf.fit(df_traindata, df_trainlabel)\n",
    "        y_pred = clf.predict(df_testdata)\n",
    "        # algo.append(clf.__class__.__name__)\n",
    "        scores.append(accuracy_score(y_pred, df_testlabel))\n",
    "    warnings.filterwarnings('ignore')\n",
    "    Naivescore_df_Test = pd.DataFrame({'Algorithm': algo, 'Score': scores}).set_index('Algorithm')\n",
    "\n",
    "    # Bar plot between Train and Test Accuracy\n",
    "    fig = plt.figure(figsize=(5, 5))  # Create matplotlib figure\n",
    "\n",
    "    ax = fig.add_subplot(111)  # Create matplotlib axes\n",
    "    ax2 = ax.twinx()  # Create another axes that shares the same x-axis as a\n",
    "    width = .3\n",
    "\n",
    "    Naivescore_df_Train.Score.plot(kind='bar', color='green', ax=ax, width=width, position=0)\n",
    "    Naivescore_df_Test.Score.plot(kind='bar', color='red', ax=ax2, width=width, position=1)\n",
    "\n",
    "    ax.grid(None, axis='y')\n",
    "    ax2.grid(None)\n",
    "\n",
    "    ax.set_ylabel('Train')\n",
    "    ax2.set_ylabel('Test')\n",
    "\n",
    "    ax.set_xlim(-1, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_parameters_estimation(X_train, X_test, y_train, y_test, scores, pipe, grid_parameters, n_cv = 5):\n",
    "\n",
    "    # Creates an empty dictionary with best parameters for every scoring parameter considered\n",
    "    best_parameters = {key: None for key in scores}\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(24,20), dpi=100)\n",
    "    fig.suptitle(\"Hyperparameters selection: results\")\n",
    "        \n",
    "    # Cross-validation **for every score!**\n",
    "    for score, ax in zip(scores, ax.reshape(-1)):\n",
    "        print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "        print()\n",
    "\n",
    "        clf = GridSearchCV(\n",
    "            estimator = pipe,\n",
    "            param_grid = grid_parameters,\n",
    "            scoring = score,\n",
    "            n_jobs = -1,\n",
    "            verbose = 4,\n",
    "            return_train_score = True,\n",
    "            cv = n_cv\n",
    "            )\n",
    "\n",
    "        # ignore divide-by-zero warnings, these occur inevitably in the parameter estimation phase and are annoying\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(action=\"ignore\", category=UndefinedMetricWarning)\n",
    "            # .. your divide-by-zero code ..\n",
    "            clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "        # PLOTTING CROSS VALIDATION RESULTS:\n",
    "\n",
    "        print(\"\\nBest parameters set found on development set:\\n\")\n",
    "        print(clf.best_params_)\n",
    "        best_parameters[score] = clf.best_params_\n",
    "\n",
    "    \n",
    "        print(\"\\nGrid scores on development set:\\n\")\n",
    "        means = clf.cv_results_['mean_test_score']\n",
    "        stds = clf.cv_results_['std_test_score']\n",
    "        for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "            print(\"%r: %0.3f (+/-%0.03f) for %r\"\n",
    "                  % (score, mean, std * 2, params))\n",
    "\n",
    "\n",
    "        # FINAL CLASSIFICATION ON TEST SET WITH BEST PARAMETERS (THAT MAXIMISE THE CURRENT SCORE)\n",
    "        \n",
    "        print(\"\\nDetailed classification report:\\n\")\n",
    "        print(\"The model is trained on the full development set.\")\n",
    "        print(\"The scores are computed on the full evaluation set.\\n\")\n",
    "        \n",
    "        # PLOT RESULTS\n",
    "\n",
    "        # ignore divide-by-zero warnings, these occur inevitably in the parameter estimation phase and are annoying\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(action=\"ignore\", category=UndefinedMetricWarning)\n",
    "            # .. your divide-by-zero code ..\n",
    "            y_true, y_pred = y_test, clf.predict(X_test)\n",
    "            print(classification_report(y_true, y_pred))\n",
    "        print()\n",
    "        \n",
    "        # PLOT CONFUSION MATRIX\n",
    "\n",
    "        cm = confusion_matrix(y_test, y_pred, normalize='true')\n",
    "        sns.heatmap(cm, annot=True, fmt='.2%', cmap='icefire', ax=ax)\n",
    "        ax.set_title('Grid search results obtained trying to maximise for: ' + score.capitalize())\n",
    "\n",
    "        # set x-axis label and ticks. \n",
    "        ax.set_xlabel(\"Predicted Class\", fontsize=14)\n",
    "        ax.xaxis.set_ticklabels(['BARBUNYA', 'BOMBAY', 'CALI', 'DERMASON', 'HOROZ', 'SEKER', 'SIRA'])\n",
    "\n",
    "        # set y-axis label and ticks\n",
    "        ax.set_ylabel(\"Actual Class\", fontsize=14)\n",
    "        ax.yaxis.set_ticklabels(['BARBUNYA', 'BOMBAY', 'CALI', 'DERMASON', 'HOROZ', 'SEKER', 'SIRA'])\n",
    "\n",
    "        \n",
    "    return best_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_final_traintest(pipe, X_train, X_test, y_train, y_test, best_parameters, selected_score):\n",
    "    \n",
    "    # Print the best parameters configurations that maximise specific metric/scores\n",
    "    print(\"List of the parameters used according to a specific scoring parameter:\")\n",
    "    print(json.dumps(best_parameters, indent=4))\n",
    "    print()\n",
    "\n",
    "    # # Add a specific parameter to the configuration in order to show iterations\n",
    "    # for i in best_parameters:\n",
    "    #     best_parameters[i]['verbose'] = 0\n",
    "    #     # More info on the 'verbose' parameter:\n",
    "    #     # The verbosity level: if non zero, progress messages are printed. Above 50, the output is sent to stdout.\n",
    "    #     # The frequency of the messages increases with the verbosity level. If it more than 10, all iterations are\n",
    "    #     # reported.\n",
    "    #     # verbose: 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch\n",
    "\n",
    "    # Final classification with optimized parameters:\n",
    "    print(\"SVM Classification:\")\n",
    "    print(\"Parameters set in order to maximise the %s scoring parameter\" % selected_score.upper())\n",
    "\n",
    "    # Instantiate a new classifier object with best parameters for a selected scoring parameter\n",
    "    clf = pipe\n",
    "    clf.set_params(**best_parameters[selected_score])\n",
    "    # Alternative: creating a new classifier object with manually selected parameters, not optimized\n",
    "    # clf = SVC(kernel='rbf', decision_function_shape='ovo', verbose=2)\n",
    "\n",
    "    # Model training:\n",
    "    print(\"Model training:\")\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(action=\"ignore\", category=UndefinedMetricWarning)\n",
    "        # .. your divide-by-zero code ..\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "    # Model testing:\n",
    "    print(\"\\n\\nMaking predictions:\")\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(action=\"ignore\", category=UndefinedMetricWarning)\n",
    "        # .. your divide-by-zero code ..\n",
    "        y_predicted = clf.predict(X_test)\n",
    "\n",
    "        # Performance evaluation\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(confusion_matrix(y_test, y_predicted))\n",
    "        print()\n",
    "\n",
    "        print(classification_report(y_test, y_predicted))\n",
    "        print()\n",
    "\n",
    "    \n",
    "    # PLOT CONFUSION MATRIX\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 10))  # figure size is given as a (width, height) tuple\n",
    "    ax1 = fig.add_subplot(111)\n",
    "    cm = confusion_matrix(y_test, y_predicted, normalize='true')\n",
    "    sns.heatmap(cm, annot=True, fmt='.2%', cmap='icefire', ax=ax1)\n",
    "    ax1.set_title('Result obtained with parameters maxmimizing: ' + selected_score.capitalize())\n",
    "\n",
    "    # set x-axis label and ticks. \n",
    "    ax1.set_xlabel(\"Predicted Class\", fontsize=14)\n",
    "    ax1.xaxis.set_ticklabels(['BARBUNYA', 'BOMBAY', 'CALI', 'DERMASON', 'HOROZ', 'SEKER', 'SIRA'])\n",
    "\n",
    "    # set y-axis label and ticks\n",
    "    ax1.set_ylabel(\"Actual Class\", fontsize=14)\n",
    "    ax1.yaxis.set_ticklabels(['BARBUNYA', 'BOMBAY', 'CALI', 'DERMASON', 'HOROZ', 'SEKER', 'SIRA'])\n",
    "    \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset\n",
    "df = pd.read_excel(\"DryBeanDataset/Dry_Bean_Dataset.xlsx\")\n",
    "X = df[df.columns.tolist()[:-1]]\n",
    "y = df[\"Class\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "X_train, X_test, y_train, y_test  = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter selection (grid search and k-fold cross-validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameter optimization using k-fold cross validation: estimation of the classifier's parameters that maximise specific metrics/scores\n",
    "\n",
    "models = {\n",
    "    \"Random_Forest\": RandomForestClassifier(\n",
    "        min_samples_leaf=1, max_depth = None, bootstrap = True, \n",
    "        oob_score = False, min_samples_split = 2,\n",
    "        ),\n",
    "    \"Extra_Trees\": ExtraTreesClassifier(\n",
    "        min_samples_leaf=1, max_depth = None, bootstrap = False, \n",
    "        oob_score = False, min_samples_split = 2,\n",
    "        ),\n",
    "    \"KNN\": KNeighborsClassifier(\n",
    "        ),\n",
    "    \"SVM\": SVC(\n",
    "        ),\n",
    "    \"MLP\": MLPClassifier( \n",
    "        alpha=1e-5, \n",
    "        learning_rate='constant',\n",
    "        learning_rate_init=0.001,    \n",
    "        max_iter=1000,\n",
    "        tol=1e-4,\n",
    "        verbose=True,\n",
    "        momentum=0.9,\n",
    "        early_stopping=False,\n",
    "        )\n",
    "}\n",
    "\n",
    "# Set the parameters by cross-validation for different models\n",
    "pipe_models_parameters = {\n",
    "    \"Random_Forest\":\n",
    "        {\n",
    "            \"Random_Forest__n_estimators\": [32], \n",
    "            \"Random_Forest__criterion\": [\"gini\"],                    # {\"gini\", \"entropy\", \"log_loss\"}\n",
    "            \"Random_Forest__max_features\": [\"sqrt\"],                 # {\"sqrt\", \"log2\", None}\n",
    "        },\n",
    "    \"Extra_Trees\":   \n",
    "        {\n",
    "            \"Extra_Trees__n_estimators\":  [32], \n",
    "            \"Extra_Trees__criterion\": [\"gini\"],                    # {\"gini\", \"entropy\", \"log_loss\"}\n",
    "            \"Extra_Trees__max_features\": [\"sqrt\"],                 # {\"sqrt\", \"log2\", None}\n",
    "        },\n",
    "    \"KNN\": \n",
    "        {\n",
    "            'KNN__algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "            'KNN__n_neighbors': [5, 10, 15],\n",
    "        },\n",
    "    \"SVM\": \n",
    "        [\n",
    "            {\n",
    "                'SVM__kernel': ['rbf'],\n",
    "                'SVM__gamma': [0.05, 0.1],\n",
    "                'SVM__C': [100],\n",
    "                'SVM__decision_function_shape': ['ovo'],  # by default this is set to ovr\n",
    "            },   \n",
    "    \n",
    "            {   \n",
    "                'SVM__kernel': ['linear'],\n",
    "                'SVM__C': [1, 10, 100, 1000],\n",
    "                'SVM__decision_function_shape': ['ovo'],\n",
    "            }\n",
    "        ],\n",
    "    \"MLP\":\n",
    "            {\n",
    "                'MLP__activation': ['relu', 'sigmoid', 'tanh'],\n",
    "                'MLP__solver': ['adam', 'sgd'],\n",
    "            }\n",
    "}\n",
    "\n",
    "# Choose the metrics to optimize for\n",
    "scores = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro']\n",
    "\n",
    "model_results  = {\n",
    "    \"Random_Forest\": {},\n",
    "    \"Extra_Trees\": {},\n",
    "    \"KNN\": {},\n",
    "    \"SVM\": {},\n",
    "    \"MLP\": {}\n",
    "}\n",
    "\n",
    "for model in models:\n",
    "# Create pipeline\n",
    "    pipe = Pipeline([['scaler', StandardScaler()], ['over', SMOTE()], [model, models.get(model)]])  # [None, SMOTE(), ADASYN(), SMOTETomek(), SMOTEENN()]\n",
    "    model_best_parameters = pipeline_parameters_estimation(X_train, X_test, y_train, y_test, scores, pipe, model, n_cv=5)\n",
    "    model_results[model] = model_best_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'svc_classification' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/samuponz/Desktop/drybeans/svm_classification.ipynb Cell 14\u001b[0m in \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/samuponz/Desktop/drybeans/svm_classification.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Final classification (on test set)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/samuponz/Desktop/drybeans/svm_classification.ipynb#X16sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/samuponz/Desktop/drybeans/svm_classification.ipynb#X16sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# selection of a particular configuration: this is done selecting the relative metric\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/samuponz/Desktop/drybeans/svm_classification.ipynb#X16sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m selected_score \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/samuponz/Desktop/drybeans/svm_classification.ipynb#X16sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m svm_clf \u001b[39m=\u001b[39m svc_classification(pipe, X_train, X_test, y_train, y_test, best_parameters, selected_score)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'svc_classification' is not defined"
     ]
    }
   ],
   "source": [
    "# Final classification (on test set)\n",
    "\n",
    "# selection of a particular configuration: this is done selecting the relative metric\n",
    "selected_score = \"accuracy\"\n",
    "\n",
    "best_parameters = \n",
    "\n",
    "svm_clf = pipeline_final_traintest(pipe, X_train, X_test, y_train, y_test, best_parameters, selected_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Create a feature-selection transform, a scaler and an instance of SVM that we\n",
    "# combine together to have a full-blown estimator\n",
    "\n",
    "clf = Pipeline(\n",
    "    [\n",
    "        (\"anova\", SelectPercentile(f_classif)), # Metric is ANOVA\n",
    "        (\"scaler\", MinMaxScaler()),\n",
    "        (\"svc\", SVC(gamma=\"auto\")),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This takes 2 minutes\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "score_means = list()\n",
    "score_stds = list()\n",
    "percentiles = (1, 3, 6, 10, 15, 20, 30, 40, 60, 80, 100)\n",
    "\n",
    "for percentile in percentiles:\n",
    "    clf.set_params(anova__percentile=percentile)\n",
    "    this_scores = cross_val_score(clf, X, y)\n",
    "    score_means.append(this_scores.mean())\n",
    "    score_stds.append(this_scores.std())\n",
    "\n",
    "plt.errorbar(percentiles, score_means, np.array(score_stds))\n",
    "plt.title(\"Performance of the SVM-Anova varying the percentile of features selected\")\n",
    "plt.xticks(np.linspace(0, 100, 11, endpoint=True))\n",
    "plt.xlabel(\"Percentile\")\n",
    "plt.ylabel(\"Accuracy Score\")\n",
    "plt.axis(\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments\n",
    "\n",
    "It seems with both standard and minmax scaling, the accuracy increments using more features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NAIVE CLASSIFICATION: multiple classifiers compared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison: Random Forests and KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_classification(x, y):\n",
    "\n",
    "    # Splitting the dataset:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Feature Scaling\n",
    "    sc = StandardScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform(X_test)\n",
    "\n",
    "    # # Quick implementation: --------------------------------------------\n",
    "    # # Create a Gaussian Classifier\n",
    "    # clf = RandomForestClassifier(n_estimators=100)\n",
    "    #\n",
    "    # # Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "    # clf.fit(X_train, y_train)\n",
    "    #\n",
    "    # y_pred = clf.predict(X_test)\n",
    "    # # Model Accuracy, how often is the classifier correct?\n",
    "    # print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    # print()\n",
    "    # # ------------------------------------------------------------------\n",
    "\n",
    "    # Set the parameters by cross-validation\n",
    "    tuned_parameters = [\n",
    "        {'n_estimators': [350, 400, 450, 500],\n",
    "            'max_depth': [2, 5, 7, 9, 20, 50, 100]}\n",
    "    ]\n",
    "\n",
    "    scores = ['accuracy']\n",
    "    # scores = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro']\n",
    "\n",
    "    # Creates a dictionary with best parameters for every scoring parameter considered\n",
    "    # best_parameters = {key: None for key in scores}\n",
    "\n",
    "    for score in scores:\n",
    "        print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "        print()\n",
    "\n",
    "        clf = GridSearchCV(\n",
    "            RandomForestClassifier(), tuned_parameters, scoring=score, cv=5\n",
    "        )\n",
    "\n",
    "        # ignore divide-by-zero warnings, these occur inevitably in the parameter estimation phase and are annoying\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(action=\"ignore\", category=UndefinedMetricWarning)\n",
    "            # .. your divide-by-zero code ..\n",
    "            clf.fit(X_train, y_train)\n",
    "\n",
    "        print(\"Best parameters set found on development set:\")\n",
    "        print()\n",
    "        print(clf.best_params_)\n",
    "        print()\n",
    "        print(\"Grid scores on development set:\")\n",
    "        print()\n",
    "        means = clf.cv_results_['mean_test_score']\n",
    "        stds = clf.cv_results_['std_test_score']\n",
    "        for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "            print(\"%r: %0.3f (+/-%0.03f) for %r\"\n",
    "                  % (score, mean, std * 2, params))\n",
    "        print()\n",
    "\n",
    "        print(\"Detailed classification report:\")\n",
    "        print()\n",
    "        print(\"The model is trained on the full development set.\")\n",
    "        print(\"The scores are computed on the full evaluation set.\")\n",
    "        print()\n",
    "\n",
    "        # ignore divide-by-zero warnings, these occur inevitably in the parameter estimation phase and are annoying\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(action=\"ignore\", category=UndefinedMetricWarning)\n",
    "            # .. your divide-by-zero code ..\n",
    "            y_true, y_pred = y_test, clf.predict(X_test)\n",
    "            print(classification_report(y_true, y_pred))\n",
    "        print()\n",
    "\n",
    "        print(clf.best_params_)\n",
    "        # best_parameters[score] = clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_classification(x, y):\n",
    "\n",
    "    # Splitting the dataset:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Feature Scaling\n",
    "    sc = StandardScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform(X_test)\n",
    "\n",
    "    # # Quick implementation: --------------------------------------------\n",
    "    # # Create a Gaussian Classifier\n",
    "    # clf = RandomForestClassifier(n_estimators=100)\n",
    "    #\n",
    "    # # Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "    # clf.fit(X_train, y_train)\n",
    "    #\n",
    "    # y_pred = clf.predict(X_test)\n",
    "    # # Model Accuracy, how often is the classifier correct?\n",
    "    # print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    # print()\n",
    "    # # ------------------------------------------------------------------\n",
    "\n",
    "    # Set the parameters by cross-validation\n",
    "    tuned_parameters = [\n",
    "        {'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "            'n_neighbors': [5, 10, 15]}\n",
    "    ]\n",
    "\n",
    "    scores = ['accuracy']\n",
    "    # scores = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro']\n",
    "\n",
    "    # Creates a dictionary with best parameters for every scoring parameter considered\n",
    "    # best_parameters = {key: None for key in scores}\n",
    "\n",
    "    for score in scores:\n",
    "        print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "        print()\n",
    "\n",
    "        clf = GridSearchCV(\n",
    "            KNeighborsClassifier(), tuned_parameters, scoring=score, cv=5\n",
    "        )\n",
    "\n",
    "        # ignore divide-by-zero warnings, these occur inevitably in the parameter estimation phase and are annoying\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(action=\"ignore\", category=UndefinedMetricWarning)\n",
    "            # .. your divide-by-zero code ..\n",
    "            clf.fit(X_train, y_train)\n",
    "\n",
    "        print(\"Best parameters set found on development set:\")\n",
    "        print()\n",
    "        print(clf.best_params_)\n",
    "        print()\n",
    "        print(\"Grid scores on development set:\")\n",
    "        print()\n",
    "        means = clf.cv_results_['mean_test_score']\n",
    "        stds = clf.cv_results_['std_test_score']\n",
    "        for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "            print(\"%r: %0.3f (+/-%0.03f) for %r\"\n",
    "                  % (score, mean, std * 2, params))\n",
    "        print()\n",
    "\n",
    "        print(\"Detailed classification report:\")\n",
    "        print()\n",
    "        print(\"The model is trained on the full development set.\")\n",
    "        print(\"The scores are computed on the full evaluation set.\")\n",
    "        print()\n",
    "\n",
    "        # ignore divide-by-zero warnings, these occur inevitably in the parameter estimation phase and are annoying\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(action=\"ignore\", category=UndefinedMetricWarning)\n",
    "            # .. your divide-by-zero code ..\n",
    "            y_true, y_pred = y_test, clf.predict(X_test)\n",
    "            print(classification_report(y_true, y_pred))\n",
    "        print()\n",
    "\n",
    "        print(clf.best_params_)\n",
    "        # best_parameters[score] = clf.best_params_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
